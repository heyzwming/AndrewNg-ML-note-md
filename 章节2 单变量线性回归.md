章节2 单变量线性回归
===

## Model and Cost Function

## 课时6  模型描述   08:10
上一章已经通过卖房价格的模型简单介绍了什么是回归：我们尝试将变量映射到某一个连续函数上。

![6.1](http://m.qpic.cn/psb?/V12umJF70r2BEK/CafuXmKosDXN.5FMGtIEq7n6ocrA1dXgyhPeYsX6wuI!/b/dDwBAAAAAAAA&bo=IQcDBAAAAAARBxE!&rf=viewer_4)

这章我们将这个问题简单地量化为**单变量线性回归模型**(Univariate linear regression)来理解它。

我们有一堆数据集，也叫训练集，下图我们来定义一些课程中用到的符号。

首先，我们定义三个变量：

m = 用于训练的样本数
x^i = 第i个训练样本“输入”变量/特征量
​y^i = ​第i个训练样本“输出”变量/特征量
![6.2](http://m.qpic.cn/psb?/V12umJF70r2BEK/*ssrGbJhFGJCR0xMuxqlXZNyH.p.tXpTg3dWkqjX30o!/b/dIABAAAAAAAA&bo=NgcABAAAAAARFxU!&rf=viewer_4)

如何给训练集下定义，先来看一下监督学习算法是怎么工作的.

算法的任务是 输出一个函数，用小写字母h表示,h表示假设(hypothesis)函数,这个假设函数的作用是把房子的大小作为输入变量x值,并输出想应房子的预测y值。

接下来人们的问题变成了如何表示假设函数

![6.3](http://m.qpic.cn/psb?/V12umJF70r2BEK/h0A6gdlaZzTGT3IvEPpZHyFSAihJUIvzfNCyPbnxvl8!/b/dIUBAAAAAAAA&bo=Swf8AwAAAAARF5M!&rf=viewer_4)

以及一个函数：

h_θ(x)=θ_0+θ_1*x               (1.1)
其中h是hypothesis（假设）的意思，当然，这个词在机器学习的当前情况下并不是特别准确。θ是参数，我们要做的是通过训练使得θ的表现效果更好。
这种模型被称为线性回归/单变量线性回归(Univariate linear regression)。

## 课时7  代价函数   08:12

> 本节中我们将定义代价函数的概念，这有助于我们弄清楚如何把最有可能的直线与我们的数据相拟合

在这个假设函数h中，θ_i 我们把他们陈伟模型参数，我们要做的就是如何选择这两个参数值θ_0和θ_1.

![7.1](http://m.qpic.cn/psb?/V12umJF70r2BEK/5FCPtEHttxyY9q5doh3MhFKYsLCsg*BuZY2dy4T1ftg!/b/dIUBAAAAAAAA&bo=CQfoAwAAAAARB9U!&rf=viewer_4)

不同的θ有不同的假设和不同的假设函数。

![7.2](http://m.qpic.cn/psb?/V12umJF70r2BEK/icxayOXaOTc*FZZgoSudFYNi5FTs.M1ohkvRTMBhiG8!/b/dH4BAAAAAAAA&bo=Nwe9AwAAAAARF64!&rf=viewer_4)

我们现在有了数据集，并且可以通过改变参数来调整h函数，那么，我们如何定义什么是“更好”的h函数呢？
> 让我们给出标准的定义：在线性回归中，我们要解决的是一个最小化问题,所以我们要写出关于θ_0和θ_1的最小化，而且想要h(x)和y之间的差异尽可能小。即：通过调整θ，使得所有训练集数据与其拟合数据的差的平方和更小，即认为得到了拟合度更好的函数。

我们引入了代价函数(平方误差函数/平方误差代价函数)：
![代价函数](http://m.qpic.cn/psb?/V12umJF70r2BEK/79Anu8a5sjwWb*iqijw6Ld*WNzrw9qN*zCLjt63TnPg!/b/dH4BAAAAAAAA&bo=kwJuAAAAAAARF98!&rf=viewer_4)

注意，其中的12m取2m而非m当系数是为了后面梯度下降算法里，求导后消掉2.

当代价函数J最小的时候（​minimize  J(θ0,θ1)​），即找到了对于当前训练集来说拟合度最高的函数h。

![7.3](http://m.qpic.cn/psb?/V12umJF70r2BEK/XbJqwpVJTFTQMKOLdprtzZVqbY7VUq.ovRVREXtUNx4!/b/dPQAAAAAAAAA&bo=LAcfBAAAAAARFxA!&rf=viewer_4)

对于我们研究的单变量线性回归而言，J函数关于θ的等高线图像大致如下：



当我们找到了这些同心椭圆的中心点时，就找到了J函数的最小值，此时拟合度更好。

## 课时8  代价函数（一） 11:09


## 课时9  代价函数（二） 08:48

## Parameter Learning

## 课时10  梯度下降  11:30


## 课时11  梯度下降知识点总结    11:50


## 课时12  线性回归的梯度下降    10:20



## 课时13  本章课程总结

