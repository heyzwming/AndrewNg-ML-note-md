十六、Recommender Systems(推荐系统)
===
## Predicting Movie Ratings()
---
## 16.1、 Problem Formulation(问题规划)

我们以预测电影评分系统为例，进行讨论：

现在我们在院线上有五部电影，并且有四个观众的评分数据，评分从一星到五星(这里讨论零到五星的情况)，问号表示观众有可能并没看该电影，也就是未评分。

!

在这里，我们给出如下定义：

​$u$=​用户的数量
​$m$=​电影的数量
$n_u$=用户的编号(no.users)
$n_m$=电影的编号(no.movies)
$​r(i,j)=1$ 如果用户j对电影i进行了评分，​
​$y(i,j)$=1​用户j对电影i的评分（只有r(i,j)=1时才有效）。


有3部电影是爱情或爱情喜剧片 还有2部动作片 似乎 Alice 和 Bob 把高评分给了有关爱情的电影而给动作片评分很低 对于 Carol 和 Dave 正好相反 Carol 和 Dave 3号和4号用户 非常喜欢动作片 给了这些电影高评分 但是他们 对浪漫爱情类型的电影 不那么喜欢 

在推荐系统问题中 我们已知这些数据：r(i, j) 如果用户i给电影j评过分 那么r(i, j)等于1，当 r(i, j) 等于1时 也就是当用户j给电影i评过分时 我们还有这个数值 y(i, j) 它表示用户 j 给电影 i 的评分 这样 y(i, j) 将会是 一个0至5的数 取决于星级评分 也就是那位用户给那部电影 评出的0至5颗星 

于是推荐系统问题就是给定这些 r(i, j) 和 y(i, j) 数值 然后浏览全部数据 关注所有没有电影评分的地方 并试图预测这些带问号的地方应该是什么数值 

于是我们开发一个推荐系统 主要工作就是想出一种学习算法能够帮我们自动地填上这些缺失的数值 这样我们就能看一下 用户还没看过哪些电影 然后向用户推荐新电影

这些就是推荐系统问题的正式表述 

## 16.2、 Content Based Recommendations(基于内容的推荐算法)

我们先引入两个特征量​x1​和x2​，它们分别代表一部电影的“浪漫程度”、“动作精彩程度”，变化范围0-1.

一个方法是，我们对每个用户单独使用一次线性回归。对于用户j的数据而言，学习得到一组参数​$θ^{(j)}∈R^3$​。然后预测用户j对电影i的评星​$(θ^{(j)})^Tx^{(i)}$​。

​$θ^{(j)}$=​用户j的参数向量
​$x^{(i)}$=​电影i的特征向量
对于用户j，电影i，预测评星：$​(θ^{(j)})^Tx^{(i)}$

m^{(i)}=用户j评过星的电影数​
为了学习​θ^{(j)}，我们需要进行如下操作：

$$min_{θ(j)}=\frac{1}{2} \sum_{i:r(i,j)=1}((θ^{(j)})^T(x^{(i)})−y^{(i,j)})^2+\frac{\lambda}{2}\sum_{k=1}^n(θ^{(j)}_{k})^2$$

这正是我们所熟悉的线性回归。第一项求和的下标是为了选出所有使r(i,j)=1的i。

为了得到所有用户的参数，我们可以这样做：

$$min_{θ^{(1)},…,θ^{(n_u)}}=\frac{1}{2}\sum_{j=1}^{n_u} \sum_{i:r(i,j)=1}((θ^{(j)})^T(x^{(i)})−y^{(i,j)})^2+\frac{\lambda}{2}\sum_{j=1}^{n_u}\sum_{k=1}^n(θ^{(j)}_{k})^2$$

接下来我们能够使用线性回归梯度下降算法来得出上面代价函数的最优解。

唯一不同的是，我们消除了常数​1/m​.




---

在过去的视频中 我们谈到 推荐系统的问题 举个例子 假如你有一些电影 还有一些观众 他们每个人都对某些电影 进行了一些评价打分 把电影按照1星到5星  或者0到5星评分 我想做的 是通过这些 用户的评价来预测出 他们会怎样给还没看过的电影打分 在这段视频中 我想介绍第一种构造 推荐系统的方法 这种方法叫做“基于内容的推荐” 

这是我们之前的数据集 我想提醒一下 关于这个符号表示 我是用n_u来表示 用户的数量 在这里等于4 

n_m来表示电影的数量 这里是5部电影 

那么我应该如何预测 这些缺少的值呢？ 

我们假设 对这些电影的每一部 我都用一些特征来描述 具体来说 我们假设每部电影有两种特征 

分别用x1和x2代表 x1表示这部电影 属于爱情电影的程度 x2表示这部电影 是动作电影的程度 因此 对于电影 《爱到最后》 那么这部电影 是爱情电影的比率为0.9 这是一部绝对的爱情电影 但是它属于动作电影的比率为0 表示几乎没有动作内容 《浪漫永远》 爱情比率1.0 大部分都是爱情内容 动作比率0.01 我也不知道为什么 可能出了一场车祸什么的 所以有一丁点动作成分 我们跳过一些 

看最后这个 《剑与空手道》 爱情比率为0 表示没有爱情成分 估计全片都是动作情节  同样的 《无尽狂飙》 可能里面有一丁点爱情 但主要是动作 《小爱犬》 也算爱情电影 基本没有动作成分 

所以 如果每部电影 我们有这样的一些特征 那么可以用一个特征矩阵表示 对于电影1 我们暂且用电影1,2,3,4,5来指代这些电影 所以对于电影1 《爱到最后》 我的两个特征值 分别是0.9和0 这就是特征变量 x1 和 x2 我们还是像以前一样 加一个额外的特征变量 截距特征变量 x0 其值为1 

把三个特征变量放在一起 这样我就有了特征 x(1) 这个上标括号(1) 表示这是我第一部电影的 特征向量 这个向量内部是这样的 第一个值是截距项1 然后是两个特征值0.9和0 就这样 

所以特征x(1) 就是电影《爱到最后》的特征向量 

对电影《浪漫永远》 我们有另外一个特征向量 x(2) 等等 以此类推 最后电影《剑与空手道》 有另外一个向量 x(5) 

同样地 为了跟 我们之前的符号表达 保持一致 我们仍然用 n 表示特征变量数 不包括 x0 这样 n就等于2 表示两个特征变量x1和x2 分别对应每部电影的 爱情程度 和动作程度 为了进行预测 我们可以这么做 

我们可以把对 每个观众打分的预测 当成一个独立的线性回归问题 具体来说 比如每一个用户j 我们都学习出一个参数θ(j) 在这里是一个三维向量 更一般的情况是 θ(j)为一个n+1维向量 n是特征数 不包括截距项x0 然后我们要根据参数向量θ 与特征x(i)的内积 来预测用户j 对电影i的评分 

我们来看一个具体的例子吧 

我们来看用户1 

Alice 与Alice对应的 参数向量 就应该是θ(1) 第二个用户Bob 就是跟第二个向量 θ(2)对应的 Carol则对应 另一个参数向量θ(3) 第四个用户Dave 对应另一个参数向量θ(4) 现在假如我们想预测 Alice对电影 《小爱犬》 是如何评价的 那么这部电影 

有一个参数向量x(3) x(3)是等于[1 0.99 0] 其中1是截距项 然后是两个特征 0.99和0 

假如说 对于这个例子 你已经知道Alice的 参数向量θ(1) 后面我们还会 详细讲到 这个参数是怎么得到的 但现在就假设 你已经知道了 用某种学习算法得到的 参数向量θ(1) 它的值等于 [0 5 0] 因此 我们对 这一项的预测 就等于θ(1)—— θ(1)是Alice的参数向量 ——的转置 乘以x(3) x(3)是3号电影 《小爱犬》的特征向量 因此 这两个 向量的内积 

就等于 5 × 0.99 

等于4.95 

因此我对这个值的预测 其结果将为4.95 

这看起来算是一个 比较合理的预测 

对于参数向量θ(1)来说 

因此 我们做的事情 实际上就是对每个用户 应用不同的线性回归模型 并且我们预测 Alice 会做的是 Alice 对应一个参数θ(1) 我们要用这个参数 

来预测Alice 对这部电影的评价 这个评价和电影的 爱情程度和动作程度是相关的 然后Bob Carol 和 Dave 他们每个人 都有一个不同的线性方程 来算出电影的爱情性和动作性 或者说某部电影的爱情程度 

和动作程度 这就是预测它们评分结果的方法 

我们可以把这个问题 写成如下更正式一些的形式 

我们用r(i,j)=1 来表示用户j 对电影i进行了评分 y(i,j)则表示用户j 

对电影i的评分值 

如果这个用户 对这部电影进行过评价 在前面的幻灯片中 我们还定义了θ(j) 表示用户j对应的参数向量 x(i)是某部电影i的 特征向量 那么某个用户对 某部电影的评分就是这样的 
6:47
现在 我临时介绍一个 额外的表示符号 m(j) 我们用m(j) 来表示用户j评价过的电影数 我们只在这一页中 使用m(j)这个符号 为了学习参数向量θ(j) 我们应该怎么做呢？ 这是一个基本的线性回归问题 因此我们要做的 就是选择一个参数向量θ(j) 使得预测的结果 这里 尽可能接近 我们在训练集中的观测值 

我们来写一下 为了学习出 参数向量θ(j) 我们要关于θ(j)最小化 下面这个求和值 

这个求和值 是用户j 对电影的所有评价 所以求和范围是所有的i值 即r(i,j)=1时的所有i 

这个求和序列的读法是 对所有满足 r(i,j)=1的这些i 进行求和运算 这样就求出了所有用户j 所评价过的电影 

然后我要计算的内容 

是θ(j)的转置 

乘以x(i) 这就是用户j 对电影i评分的预测值 减去y(i,j) 这是实际观测到的评分值 然后平方 

在求和式前面 除以用户j评价过的 所有电影的数量 也就是乘上1/2m(j) 这其实就很像 最小二乘回归 或者线性回归 我们要选择一个 最佳的参数θ(j)来最小化这个平方误差项 当然如果你愿意的话 

你也可以加上正则化项 加上λ/2m 

实际上应该是2m(j) 因为我们有m(j)个样本对吧？ 因为如果用户j 对这么多电影进行了评分的话 那么我们就需要这么多数据点 来拟合参数θ(j) 然后 我还是加上 我一般使用的正则化项 θk(j)的平方关于k求和 同样地 这里的求和是从k等于1到n 所以这里的θ(j) 将是一个 n+1维的向量 在我们之前那个例子中n为2 但更一般地 n应该是每一部电影的特征数 按照惯例 我们还是不对θ(0)进行正则化 我们不对偏差项 进行正则化 因为这个求和是从k等于1到n 因此 如果你对这个式子 关于θ(j)求最小值的话 你会得到一个解 你会得到一个很好的θ(j)的估计值 

预测出用户j 对电影的评分值 

对于推荐系统 我要把符号稍微变化一下 为了让数学更简单 我要去掉这个m(j) 

这就一个常数 我可以去掉这一项 不改变θ(j)的 最优化结果 所以你可以把这个式子 看成是算出整个表达式 然后乘以一个m(j)项 然后去掉这个常数项 当我最小化的时候 我还是会得到同样的θ(j) 再重复一下 我们前一页写的 这是我们的最优化目标 为了学习θ(j) θ(j)是用户j对应的参数向量 我们要关于θ(j)最小化 这个最优化目标 这是我们通常的 平方误差项 这是我们的正则化项 

当然 在构建 推荐系统的时候 我们也不想只对某一个用户 学习出参数向量 我们想对所有的用户都学习出θ 因为我有n_u个用户 所以我希望学习出 所有的参数 那么我要做的是 将这个最优化目标 另外再加上一个求和 所以 这里的表达式 前面是1/2 这实际上就跟 我们上面的一样 唯一不同的是 现在不是只对一个θ(j) 现在我要对所有的用户 求这个目标函数的和 然后对整个优化目标 求最小值 最小化整个这个代价函数 

当我关于θ(1) θ(2) 一直到θ(n_u) 最小化 这个函数时 我就会得到对每个用户 不同的参数向量 然后我可以用它们 来对所有的用户 所有这n_u个用户 来作出预测了 

把所有这些放在一起 最上面的就是 我们的最优化目标 给这个项 起一个名字吧 就叫它J(θ(1), ... ,θ(n_u)) 同样地 J还是我们要最小化的最优化目标函数 

接下来 为了 求出这个最小值 如果你想要用 梯度下降来更新的话 你可能会用到这些式子 

你可能会用 θ(j)k 减去学习速率α 乘以右边这一项 对于k=0和k≠0的情况 我们的式子有一点点区别 因为我们的正则化项 只对k不为0的θ(j)k 进行正则化 因此 我们不对 θ0进行正则化 因此这里在更新的时候 k=0和k≠0会有一点区别 

这里这一项 这实际上是 你的最优化目标函数 对你的参数 

求偏微分 对吧？ 因此 这实际上就是 梯度下降法 我已经算出了偏微分然后放在这里了 

如果你觉得这个 梯度下降的更新 看起来跟之前 线性回归差不多的话 那是因为这其实就是线性回归 唯一的一点区别 是在线性回归中 我们有1/m项 实际上这里我们也有 1/m(j)项 但在前面我们 推导最优化目标函数时 我们忽略了这个项 因此这里就没有了 除此之外 实际上就是 对所有的训练样本求和 预测误差乘以 xk 加上正则化项 

组成了这个导数项 

所以 如果你用 梯度下降法的话 你可以这样 最小化代价函数J 来学习出所有的参数 当然 用这些微分项 如果你愿意的话 你也可以把它们用在 更高级的优化算法里 比如聚类下降 或者L-BFGS(Limited-memory Broyden–Fletcher–Goldfarb–Shanno Algorithm) 或者别的方法 来最小化代价函数J 

通过这节课 你应该知道了 怎样应用一种 事实上是线性回归的一个变体 来预测不同用户对不同电影的评分值 这种具体的算法叫 ”基于内容的推荐“ 或者”基于内容的方法“ 因为我们假设 我们有不同电影的特征 我们有了电影 内容的特征 比如电影的爱情成分有多少？ 

动作成分有多少？ 我们就是用电影的这些特征 来进行预测 

但事实上 对很多电影 我们并没有这些特征 或者说 很难得到 所有电影的特征 很难知道 我们要卖的产品 有什么样的特征 

## Collaborative Filtering(协同过滤)
---
## 16.3、 Collaborative Filtering(协同过滤)


我们如果想要知道一部电影的“浪漫程度”、“动作精彩程度”，可以通过用户的偏好参数以及其对于该电影的评价来推测。

如果用户没有给定我们他的偏好系数该怎么办呢？这时候就需要反过来用电影的特征来推断其偏好参数了。

!   

这样子反复不断的相互影响，虽然可行，但是显得有些麻烦。那么，我们有没有办法同时计算出两者呢？

这就引入了协作过滤算法：

!

我们看到，它们互相影响对方的表达式中，第一项是一样的，区别在于第二项θ和x，那么我们可以综合两者得出新的最小化代价函数目标：

$$J(x,θ)=\frac{1}{2} \sum​ _{(i,j):r(i,j)=1} ​((θ^{(j)})^Tx(i)−y^{(i,j)})^2+2λ​\sum_{i=1}^{n_m} ​\sum_{k=1}^n​(x^{(i)}_k​)^2+2λ​ \sum_{j=1}^{n_u} ​ ​\sum_{k=1}^n​(θ^{(j)}_k​)^2$$

其中的偏置单元​x0=1​被移除了，因此x和θ都是n维实空间。

算法步骤如下：

初始化​$x^{(i)}​,…,​x^{(n_m)}​,θ^{(1)}​,…,θ^{(n_u)}​$为随机小量。之所以这样而不是统一置零，是为了打破平衡确保$x^{(i)}​,…,​x^{(n_m)}​$相互不同。
使用梯度下降算法（或者高级的优化算法）来最小化$​J(x^{(i)},…,x^{(n_m)},θ^{(1)},…,x^{(n_u)})$​。


$$x^{(i)}_k​:=x^{(i)}_k​−α \{ ​\sum_{j:r(i,j)=1}​((θ^{(j)})^Tx^{(i)}−y^{(i,j)})θ^{(j)}_k​+λx^{(i)}_k​ \} $$


$$\theta^{(i)}_k​:=\theta^{(i)}_k​−α \{ ​\sum_{i:r(i,j)=1}​((θ^{(j)})^Tx^{(i)}−y^{(i,j)})x^{(j)}_k​+λ\theta^{(i)}_k​ \} $$

使用一个用户的偏好参数θ和电影的特征系数x来预测该用户未评价过的电影的评星​θTx​。



---


在这段视频中 我们要讲 一种构建推荐系统的方法 叫做协同过滤(collaborative filtering) 

我们所讲的算法 有一个值得一提的 特点 那就是它能实现 对特征的学习 我的意思是 这种算法能够 自行学习所要使用的特征 

我们建一个数据集 假定是为每一部电影准备的 对每一部电影 我们找一些人来 告诉我们这部电影 浪漫指数是多少 动作指数是多少 

但想一下就知道 这样做难度很大 也很花费时间 你想想 要让每个人 看完每一部电影 告诉你你每一部电影有多浪漫 多动作 这是一件不容易的事情 而且通常 你还会希望得到除这两个特征之外的其他指数 那么你怎样才能得到这些特征呢？ 

所以 让我们转移一下问题 假如我们 有某一个数据集 我们并不知道特征的值是多少 所以比如我们得到一些 关于电影的数据 不同用户对电影的评分 我们并不知道每部电影 到底有多少浪漫的成分 也不知道到底每部电影里面动作成分是多少 于是我把所有的问题都打上问号 

现在我们稍稍改变一下这个假设 

假设我们采访了每一位用户 而且每一位用户都告诉我们 他们是否喜欢 爱情电影 以及 他们是否喜欢动作电影 这样 Alice 就有了对应的参数 θ(1) Bob 的是 θ(2) Carol 的是 θ(3) Dave 的是 θ(4) 我们还有这样的假设 假如 Alice 告诉我们 她十分喜欢 爱情电影 于是 Alice 的特征 x1 对应的值就是5 假设 Alice 告诉我们 她非常不喜欢动作电影 于是这一个特征就是0 

Bob 也有相似的喜好 

所以也就有了 θ(2) 的数据 但 Carol 说 

她非常喜欢动作电影 于是这个特征就被记录为5 也就是 x2 的值 别忘了 

我们仍然有等于1的 x0 假设 Carol 告诉我们 她不喜欢爱情电影之类的 而且戴夫也是这样 于是 我们假定 某种程度上 我们就可以着眼于用户 看看任意的用户 j 对应的 θ(j) 是怎样的 这样就明确地告诉了我们 他们对不同题材电影的喜欢程度 

如果我们能够从用户那里 得到这些 θ 参考值 那么我们理论上就能 推测出每部电影的  x1 以及 x2 的值 

我们来看个例子 假如我们看电影1 

于是电影1就对应于 表示特征的向量 x1 联系在一起了 这部电影的名字叫《爱到最后》 但这不重要 假设我们不知道 这部电影的主要内容 所以也不要在意电影的名字 

我们知道的就是 Alice 喜欢这部电影 Bob 喜欢这部电影 Carol 和 Dave 不喜欢它 

那么我们能推断出什么呢？ 好的 我们从 特征向量知道了 Alice 和 Bob 喜欢爱情电影 因为他们都在这里评了5分 然而 Carol 和 Dave 我们知道他们不喜欢 爱情电影 但喜欢动作电影 由于你知道这些 是可以从 第3和第4个参数看出来的 

同时 由于我们知道 Alice 和 Bob 喜欢电影1 而 Carol 和 Dave 不喜欢它 我们可以推断 这可能是一部爱情片 而不太可能是动作片 

这个例子在数学上 可能某种程度上简化了 但我们真正需要的是 特征向量 x(1) 应该是什么 才能让 θ(1) 的转置 乘以x(1) 约等于5 也就是 Alice 的评分值 然后 θ(2) 的转置乘以 x(1) 也近似于5 

而 θ(3) 的转置 乘以 x(1) 约等于0 这是 Carol 的评分 

而 θ(4) 的转置乘以 x(1) 也约等于0 由此可知 x(1) 应该用 [1 1.0 0.0] 这个向量表示 第一个1 是截距项 这样才能得出 Alice Bob Carol 和 Dave 四个人 对电影评分的结果 

由此及之 我们可以 继续列举 试着 弄明白 其他电影的合理特征 

让我们将这一学习问题标准化到任意特征 x(i) 假设我们的用户 告诉了我们的偏好 就是说用户们 已经给我们提供了 θ(1) 到 θ(nu) 的值 θ(1) 到 θ(nu) 的值 而我们想知道 电影 i 的 特征向量 x(i) 我们能做的 是列出以下的最优化的问题 所以 我们想要把 所有指数 j 相加 得到对电影 i 的评分 因为我们 想要求得电影 i 的特征 也就是向量 x(i) 

所以现在我们 要做的是最小化这个平方误差 我们要选择 特征 x(i) 使得 我们预测的用户 j 对该电影 i 评分的预测值评分值 跟我们从用户 j 处 实际得到的评分值 不会相差太远 也就是这个差值 不要太大 

所以 总结一下 这一阶段要做的 就是为所有 为电影评分的 用户 j 选择特征 x(i) 这一算法同样也预测出一个值 表示该用户将会如何评价某部电影 而这个预测值 在平方误差的形式中 与用户对该电影评分的实际值尽量接近 

这就是那个平方误差项了 和之前一样 我们可以加上一个正则化项 来防止特征的数值 变得过大 

这就是我们 如何从一部特定的电影中 学习到特征的方法 但我们要做的是 学习出所有电影的 所有特征 所以我现在要做的是 在此加上另外的一个求和 我要对所有的电影 nm 求和 n 下标 m 个电影 然后最小化整个这个目标函数 针对所有的电影 这样你就会得到如下的最优化的问题 

如果你将这个最小化 就应该能得到所有电影的 一系列合理的特征 

好的 把我们 前一个视频讨论的算法 以及我们刚刚 在这个视频中讲过的算法合在一起 上一个视频中 我们讲的是 如果你有一系列 对电影的评分 那么如果你 有r(i,j) 和 y(i,j) 也就是对电影的评分 

于是 根据不同电影的特征 我们可以得到参数 θ 这样 如果你知道了特征 你就能学习出不同用户的 参数 θ 值 

我们之前 这个视频中讲的是 如果用户愿意 为你提供参数 那么你就 可以为不同的电影估计特征 

这有点像鸡和蛋的问题 到底先有鸡还是先有蛋？ 就是说 如果我们能知道 θ 就能学习到 x 如果我们知道 x 也会学出 θ 来 

而这样一来 你能做的 就是 如果这真的可行的话 实际上你能做的就是 随机猜de θ 的值 

基于你一开始随机 猜测出的 θ 的值 继你可以继续下去 运用我们刚刚讲到的 步骤 我们可以学习出 不同电影的特征 

给出已有的一些电影的 原始特征 你可以运用 我们在上一个视频中讨论过的 第一种方法 可以得到 对参数 θ 的更好估计 这样就会为用户提供更好的参数 θ 集 我们就可以用这些 得到更好的 特征集或者其他数据 然后我们可以继续 迭代 不停重复 优化θ x θ x θ 这非常有效 如果你 这样做的话 你的算法将会收敛到 一组合理的电影的特征 以及一组对合理的 对不同用户参数的估计 

这就是基本的协同过滤算法 这实际并不是最后 我们将要使用的算法 下一个视频中 我们将改进这个算法 让其在计算时更为高效 但是这节课希望能让你 基本了解如何 构建一个问题 在这个问题中 从不同的电影处学到参数以及特征 

对于这个问题 对于推荐系统 可能就根据每个用户 对多部电影的评分 以及每部电影由 由不同用户的评分 这样你就可以反复进行这样的过程 来估计出 θ 和 x 总结一下 在这个视频中 我们了解了最基本的协同过滤算法 协同过滤算法指的是 当你执行这个算法时 你通过一大堆用户 得到的数据 这些用户实际上在高效地 进行了协同合作 来得到每个人 对电影的评分值 只要用户对某几部电影进行评分 每个用户就都在帮助算法 更好的学习出特征 

这样 通过自己 对几部电影评分之后 

我就能帮助系统更好的学习到特征 这些特征可以 被系统运用 为其他人 做出更准确的电影预测 协同的另一层意思 是说每位用户 都在为了大家的利益 学习出更好的特征 这就是协同过滤 在下一个视频中 我们要把这些想法 付诸实施 尝试开发一种 更完美的算法 为协同过滤算法做出一点改进


## 16.4、 Collaborative Filtering Algorithm(协同过滤算法)

给定一个矩阵X（每一行包含一部电影的特征系数）和矩阵θ（每一行包含一个用户的偏好参数），那么矩阵$​Y=XΘ^T$​代表了所有用户对所有电影的评星。

如果想要判断两部电影的相似程度，可以使用它们的特征向量x。计算公式为​
$\begin{Vmatrix}
    x^{(i)}-x^{(j)}
\end{Vmatrix}$

---

在前面几个视频里 我们谈到几个概念 首先 如果给你几个特征表示电影 我们可以使用这些资料去获得用户的参数数据 第二 如果给你用户的参数数据 你可以使用这些资料去获得电影的特征 本节视频中 我们将会使用这些概念 并且将它们合并成 协同过滤算法 (Collaborative Filtering Algorithm) 

我们之前做过的事情 其中之一是 假如你有了电影的特征 你就可以解出 这个最小化问题 为你的用户找到参数 θ 然后我们也 知道了 如果你拥有参数 θ 你也可以用该参数 通过解一个最小化问题 去计算出特征 x 

所以你可以做的事 是不停地重复这些计算 或许是随机地初始化这些参数 然后解出 θ 解出 x 解出 θ 解出 x 但实际上呢 存在一个更有效率的算法 让我们不再需要再这样不停地 计算 x 和 θ 而是能够将 x 和 θ 同时计算出来 下面就是这种算法 我们所要做的 是将这两个优化目标函数 给合为一个 所以我要来定义 这个新的优化目标函数 J 它依然是一个代价函数 是我特征 x 和参数 θ 的函数 它其实就是上面那两个优化目标函数 但我将它们给合在一起 

为了把这个解释清楚 首先 我想指出 这里的这个表达式 这个平方误差项 和下面的这个项是相同的 可能两个求和看起来有点不同 但让我们来看看它们到底到底在做什么 第一个求和运算 是所有用户 J 的总和 和所有被用户评分过的电影总和 

所以这其实是正在将 所有关于 (i,j) 对的项全加起来 表示被用户评分过的电影 关于 j 的求和 意思是 对每个用户 关于该用户评分的电影的求和 

而下面的求和运算只是用相反的顺序去进行计算 这写着关于每部电影 i 求和 关于的是 所有曾经对它评分过的 用户 j 所以这些求和运算 这两种都是对所有 (i,j) 对的求和 其中 r(i,j) 是等于1的 这只是所有你有评分的用户 和电影对而已 

因此 这两个式子 其实就是 这里的第一个式子 我已经给出了这个求和式子 

这里我写着 其为所有 r(i,j) 值为1的  (i,j) 对求和 所以我们要做的 是去定义 一个我们想将其最小化的 合并后的优化目标函数 让我们能同时解出 x 和 θ 

然后在这些优化目标函数里的 另一个式子是这个 其为 θ 所进行的正则化 它被放到这里 最后一部分 是这项式 是我 x 的优化目标函数 然后它变成这个 这个优化目标函数 J 它有一个很有趣的特性 如果你假设 x 为常数 并关于 θ 优化的话 你其实就是在计算这个式子 反过来也一样 如果你把 θ 作为常量 然后关于 x  求 J 的最小值的话 那就与第二个式子相等 因为不管是这个部分 还是这个部分 将会变成常数 如果你将它化简成只以 x 或 θ 表达的话 所以这里是 一个将我的 x 和 θ  合并起来的代价函数 

为了按照 为了解出 这个优化目标问题 我们所要做的是 将这个代价函数视为 特征 x 和用户参数 θ 的 函数 然后全部化简为 一个既关于 x 也关于 θ 的函数 

这和 前面的算法之间 唯一的不同是 不需要反复计算 就像我们之前所提到的 先关于 θ 最小化 然后关于 x 最小化 然后再关于 θ 最小化 再关于 x 最小化... 在新版本里头 不需要不断地在 x 和 θ 这两个参数之间不停折腾 我们所要做的是 将这两组参数 同时化简 

最后一件事是 当我们以这样的方法学习特征量时 之前我们所使用的 前提是 我们所使用的特征 x0 等于1 对应于一个截距 

当我们以 这种形式真的去学习特征量时 我们必须要去掉这个前提 

所以这些我们将学习的特征量 x 是 n 维实数 

而先前我们所有的 特征值x 是 n+1 维 包括截距 删除掉x0 我们现在只会有 n 维的 x 

同样地 因为参数 θ 是 在同一个维度上 所以 θ 也是 n 维的 因为如果没有 x0 那么 θ0 也不再需要 

我们将这个前提移除的理由是 因为我们现在是在 学习所有的特征 对吧? 所以我们没有必要 去将这个等于一的特征值固定死 因为如果算法真的需要 一个特征永远为1 它可以选择靠自己去获得1这个数值 所以如果这算法想要的话 它可以将特征值 x1 设为1 所以没有必要 去将1 这个特征定死 这样算法有了 灵活性去自行学习 所以 把所有讲的这些合起来 即是我们的协同过滤算法 

首先我们将会把 x 和 θ 初始为小的随机值 

这有点像 神经网络训练 我们也是将所有神经网路的参数用小的随机数值来初始化 

接下来 我们要用 梯度下降 或者某些其他的高级优化算法 把这个代价函数最小化 

所以如果你求导的话 你会发现梯度下降法 写出来的更新式是这样的 这个部分就是 代价函数 

这里我简写了 关于特征值 x(i)k 的偏微分 然后相同地 

这部分 也是代价函数 关于我们正在最小化的参数 θ 所做的偏微分 

提醒一下 这公式里  我们不再有这等于1 的 x0 项 所以 x 是 n 维 θ 也是n 维 

在这个新的表达式里 我们将所有的参数 θ 和 xn 做正则化 

不存在 θ0 这种特殊的情况 会需要不同地正则化 或者说是 跟 θ1 到 θn 的正则化 不同的 θ0 的正则化 所以现在不存在 θ0 这就是为什么 在这些更新式里 我并没有分出 k 等于0的特殊情况 

所以我们使用梯度下降 来最小化这个 代价函数 J 关于特征 x 和参数 θ 

最后 给你一个用户 如果这个用户 具有一些参数 θ 以及给你一部电影 带有已知的特征 x 我们可以预测 这部电影会被 θ 转置乘以 x 给出怎样的评分 或者将这些直接填入 那我们可以说 如果用户 j 尚未对电影 i 评分 那我们可以预测 这个用户 j 将会根据 θ(j) 转置乘以 x(i) 对电影 i 评分 

所以这就是 协同过滤算法 如果你使用这个算法 你可以得到一个十分有用的算法 可以同时学习 几乎所有电影的特征 和所有用户参数 然后有很大机会 能对不同用户会如何对他们尚未评分的电影做出评​​价 给出相当准确的预测 

## Low Rank Matrix Factorization(低秩矩阵分解)
---
## 16.5、 Vectorization:Low Rank Matrix Factorization(矢量化：低秩矩阵分解)

在前面的例子中，我们的用户至少设定了一个及以上的偏好参数。当一个新用户完全没有设置任何参数也没有任何一次评星的时候，我们该如何处理呢？

在这种情况下，如果直接进行预测，由于需要最小化代价函数，那么算法会认为新用户对所有电影的评分都是0。这样似乎并不是特别的直观与准确。

!

现在我们将通过对数据均值归一化来纠正这个问题。首先，我们使用一个矩阵Y来存储之前的评分数据，其中的第i行j列代表的是第j个用户对第i部电影的评星。

然后定义一个向量$​μ=[μ1,μ2,…,μ_{n_m}]​$

其中

$$μ_i=\frac{\sum_{j:r(i,j)=1}Y_{i,j}}{\sum_jr(i,j)}$$


这实际上是第i部电影的之前评星的平均值（其中只有用户观看过的电影才被计算在内）。我们现在可以通过从每个用户的实际评分（矩阵Y中的列）中减去μ（平均评分）来标准化数据：

例如，我们已知如下的Y矩阵和μ向量：

$$Y = \begin{bmatrix}
    5\ 5\ 0\ 0 \\
    4\ ?\ ?\ 0 \\
    0\ 0\ 5\ 4 \\
    0\ 0\ 4\ 0 \\
\end{bmatrix}$$

$$Y = \begin{bmatrix}
    2.5 \\
    2 \\
    2.25 \\
    1.25 \\
\end{bmatrix}$$



那么标准化的Y’矩阵是：

$$Y' = \begin{bmatrix}
    2.5   \ 2.5   \ -2.5 \ -2.5  \\
    2     \ ?     \ ?    \ -2    \\
    -2.25 \ -2.25 \ 3.75 \ 1.25  \\
    -1.25 \ -1.25 \ 3.75 \ -1.25 \\
\end{bmatrix}$$

现在我们在进行预测时，需要对线性回归稍加修改，添上一个平均数项：

$$(θ^{(j)})^Tx^{(i)}+μ_i$$

现在，对于新用户，我们会用平均值μ来替代零向量进行初始化，这样会显得更准确一些。

---


在上几节视频中 我们谈到了协同过滤算法 本节视频中我将会 讲到有关 该算法的向量化实现 以及说说有关该算法你可以做的其他事情 举一个例子 其中一个你可以做得是 当给出一件产品时 你能否找到与之相关的其它产品 我们不妨再举一个例子 一位用户最近看上一件产品 有没有其它相关的产品 你可以推荐给他 好的 让我们看看我们能做什么 

我将要做的是 实现一种选择的方法 写出协同过滤算法的预测情况 

首先 我们有关于五部电影 的数据集 我将要做的是 将这些用户的电影评分 进行分组并存到 一个矩阵中 看这里我们有五部电影 以及四位用户 那么 这个矩阵 Y 就是一个5行4列的矩阵 它将这些电影的用户评分数据都存在矩阵里 

包括问号标注出的 将这些数据分组到这个矩阵中 当然 矩阵中的这些元素 在(i, j)位置的元素 其实是 我们先前写的 y(i, j) 这个评分是用户 j 对电影 i 给出的评分 由这个矩阵 Y 中所有的 评分数据 就可以选择用另一种方法 写出我们对于所有的预测评分来 具体来说 如果你看到某位 用户预测 某部电影的评分 这个公式就是用户 j 对电影 i 的预测评分 

所以 如果你有 预测评分矩阵 你就会有 以下的这个 

有着(i, j)位置数据的矩阵 

它对应的评分是 我们对用户j 对电影 i 的评分的预测值 

准确说来其值等于 θ(j)转置乘x(i) 你应该也要明白 这个矩阵中的第一个元素 即我们第一行第一列的元素 是第一位用户 对第一部电影的预测分数 这个第一行第二列的 元素的预测评分 是第二位用户 对第一部电影的打分等等 下面的这个 预测评分是第一位用户 对最后一部电影的评分 你应该知道 这个评分 是我们之前对于这个值预测的结果 

同时这个评分 是我们对另外一个值预测的结果 等等 

现在 我们给出 预测评分矩阵 这里给出一个更简化的向量化的方法 具体来说 如果我定义这个矩阵 X 这就会有点 类似我们先前在线性回归里面的矩阵 

x(1)转置 

x(2)转置一直到 

x(nm)的转置 我会把这些所有 有关电影的特征 按行堆叠起来 所以 你可以把 每一部电影想象成一个范例 把这些范例即特征 按行堆叠起来 每一部电影就是一行 然后我们要做的是 定义一个大写的Θ矩阵 

接下来我要做的是将 接下来我要做的是将 每一位用户的参数向量θ(j) 像这样按行堆叠起来 这是θ(1) 是第一位用户的参数向量 

这个是θ(2) 所以你需要 像这样将这些用户的参数向量按行堆叠 以定义这个大写Θ矩阵 在这个矩阵里 

我们有nu个参数向量都像这样按行堆叠起来 

现在我们已经给出了 针对大写X矩阵以及 大写Θ矩阵的定义 为了能有 一个向量化的方法以计算 这些矩阵的预测值 你可以计算大写X矩阵乘 

大写Θ矩阵的转置 这样就给出了一种向量化的方法 以计算这个矩阵 

我们用的这个协同过滤 算法还有另外一个名字 我们现在正在使用的这个算法 也被称作是 

低秩矩阵分解 

所以如果你听到 人们说道低秩矩阵分解 准确说来 就应该是我们现在正在讲的这个算法 这一项来自 这个矩阵  乘Θ的转置 其有一个数学属性 在代数中被称为 低秩矩阵 同时  这也是对于这个算法给出 这个低秩矩阵分解名字的原因 因为这个 矩阵X乘Θ的转置的低秩属性 

如果你不懂什么是 低秩或者低秩矩阵 也不用担心 你真的不需要为了使用这个算法而知道这些 但若你对 线性代数很熟悉 那你就会知道这个算法是如何给出的 以及有关低秩矩阵分解的含义 最后 在运行了协同过滤算法后 仍有一些你可以去做的事 比如说 为了找到某部相关的电影来学习特征 

准确说来 对于每一个产品  好比每一部电影  

我们有特征向量x(i) 那么 你就会知道 当你 学习某个特征 其实并不需要知道 这些不同的特征将会变成什么样的  但如果你运行这个算法 这些特征将会完美地捕获 这些重要的方面 在你对不同电影或者产品的打分上面 到底什么是造成 某些用户 喜欢某些电影以及 是什么造成了这些电影其它不同的电影呢 也许你刚进行完 特征参数学习 你有爱情指数x1 动作指数x2 这与之前我们的视频内容类似 或许你的参数特征里有x3 是一个表现喜剧程度的指数 还有特征x4等其它参数特征 这时你就有了 n 个 参数特征 

在你完成了这些参数特征的学习后 实际中参数特征的学习 是比较困难的 并提出一个人类能理解 对于这些特征实际是什么的解释更困难 但在实践中 这些特征是很难以可视化的 也很难计算出这些特征到底是什么 

通常来说 特征学习对于 捕获哪些是 电影的重要或显著的属性 是很有意义的 也正是这样才有了你对某些电影的喜欢或不喜欢 现在我们来解决下面这个问题 

你有一部电影 i 你想找到 与电影 i 相关的 其它某部电影 j 那么是什么原因让你想用这个方法呢 可能有一位用户 正在浏览一些电影 他们当前看的电影是 i 一个什么样的理由才能让 他们看完电影 i 之后被推荐另一部电影 j 呢 或者说某人最近买了一部电影  一部什么样的其它电影 我们有理由推荐他们进行下一次购买呢 

现在既然你已经 对特征参数向量进行了学习 那么我们就会有一个很方便的方法 来度量两部电影之间的相似性 例如说 电影i有一个特征向量x(i) 你是否能找到一部 不同的电影 j 保证两部电影的特征向量 之间的距离x(i)和x(j)很小 

那就能 很有力地表明 电影 i 和电影 j 在某种程度上有相似 至少在某种意义上 某些人喜欢电影 i 或许更有可能也对电影 j 感兴趣 总结一下 当用户在看 某部电影 i 的时候 如果你想找5部 与电影 非常相似的电影 为了能给用户推荐 5部新电影 你需要做的是 找出电影 j 在这些不同的电影中 与我们要找的电影 i 的距离最小 这样你就能给你的用户推荐几部不同的电影了 通过这个方法 希望你能知道 如何进行一个向量化的计算来 对所有的用户和所有的电影 进行评分计算 同时希望你也能掌握 通过学习特征参数 来找到相关电影 和产品的方法


## 16.6、 Implementational Detail:Mean Normalization(实施细节：均值规范化)


到目前为止 你已经了解到了 推荐系统算法或者 协同过滤算法的所有要点 

在这节视频中 我想分享最后一点实现过程中的细节 

这一点就是均值归一化 有时它可以让算法 运行得更好 

为了了解均值归一化这个想法的动机 

我们考虑这样一个例子 有一个用户没有给任何电影评分 

加上之前我们有四个用户 Alice Bob Carol 和 Dave 我现在加上了第五个用户 Eve 她没有给任何电影评分 

我们来看看协同过滤算法 会对这个用户做什么 

假如说 n 等于2 所以我们要学习两个特征变量 我们要学习出 一个参数向量θ(5) 这是一个二维向量 提醒一下 这个向量是 n 维的 而不是 n+1 维的 

我们要学习5号用户 Eve 的参数向量 θ(5) 

如果我们看 这个优化目标的 第一项 用户 Eve 没给任何电影打过分 所以对用户 Eve 来说 没有电影 满足 r(i,j)=1 这个条件 所以这第一项 完全不影响 θ(5) 的值 

因为没有电影被 Eve 评过分 

所以影响 θ(5) 值的唯一一项 是这一项 这就是说 我们想选一个向量 θ(5) 使得最后的正则化项 尽可能地小 换句话说 我们想要最小化这个式子 λ/2[(θ(5)_1)^2+(θ(5)_2)^2] 

λ/2[(θ(5)_1)^2+(θ(5)_2)^2] λ/2[(θ(5)_1)^2+(θ(5)_2)^2] 它们是和用户5有关的 正则化项的要素 当然 如果你的目标是 最小化这一项 那么你最终得到的 就会是 θ(5)=[0;0] 

因为正则化项 会让你的参数 接近0 如果没有数据 能够使得参数 远离0 因为这第一项 

不影响 θ(5) 值 我们就会得到 θ(5) 等于零向量的结果 所以当我们要预测 用户5会如何 给电影打分 我们有 θ(5) 转置乘以 x(i) 

对任意i 

结果都会等于0 因为对任意x值 θ(5) 都是0 这个内积就会等于0 因此我们得到的结果是 我们会预测 Eve 给所有电影的评分 都是零星 

但是这个结果看起来没什么用吧？ 我的意思是 如果你看不同的电影 《爱到最后》 这第一个电影 两个人给它评了五星 

甚至对《剑与空手道》 也有人评了五星 所以某些人确实喜欢某些电影 看起来只是预测 Eve 会给他们全部评零星是没用的 而且实际上如果我们预测 Eve 会给所有电影零星的话 我们还是没有任何好方法 来把电影推荐给她 因为你知道 预测结果是所有这些电影 都会被 Eve 给出一样的评分 所以没有一部电影 拥有高一点儿的预测评分 让我们能推荐给她 所以这不太好 

均值归一化的想法可以让我们解决这个问题 下面介绍它是如果工作的 

和以前一样 我们把所有评分放到矩阵Y里 就是把所有这些评分 全部整合到矩阵Y中 这边这列 全部是问号的这列 对应的是 Eve 没有给任何电影评分 

现在要实现均值归一化 我要做的就是计算 每个电影所得评分的均值 我要把它们存在一个向量中 我们称这个向量为 µ 所以第一个电影得到了两个5星和两个0星的评价 均值就是2.5星评价 第二个电影的平均评价 是2.5星 等等 最后一个电影的评分是 0 0 5 0 0 0 5 0 的平均值 0 0 5 0 的平均值 就是1.25星评价 我要做的事 要把所有的电影评分 要把所有的电影评分 减去平均评分 所以这第一个元素5 我要减去2.5 等于2.5 

第二个元素5 减去2.5 得到2.5 然后是0 0 减去2.5 得到-2.5 -2.5 换句话说 我要做的就是 把我的电影评分矩阵 也就是这个Y矩阵 把它的每一行都减去那个电影的平均评分 

所以我做的就是 把每个电影都归一化为 平均评分为零 

最后一个例子 如果你看最后一行 0 0 5 0 我们要减去1.25 最后我得到 

那边这些值 那么现在 当然这些问号没变 

还是问号 所以每个电影 在新矩阵Y中的 平均评分都是0 

接下来我要做的就是 对这个评分数据集 使用协同过滤算法 所以我要假设 这就是我从用户那儿 得到的数据 或者假设它们就是 我从用户那儿得到的实际评分 我要把这个当做我的数据集 用它来学习 我的参数 θ(j) 和特征变量 x(i) 就是用这些均值归一化后的电影评分来学习 

当我想要做 电影评分预测时 我要做的步骤如下 对用户j对电影i的评分 我要预测它为 θ(j) 转置乘以 x(i) 其中 x 和 θ 都是 均值归一化的数据集中学习出的参数 但是因为我已经对数据集 减去了均值 所以为了 给电影i预测评分 我要把这个均值加回来 所以我要再加回 µi 所以我要再加回 µi 所以这就是 我得到的预测值 因为训练数据减去了所有的均值 所以当我做预测时 我们需要 

给电影 i 加回这个均值 µi 具体来说 如果用户5 Eve 之前幻灯片里的的描述仍然成立 Eve 从来没有 给任何电影打分 所以学习到的用户5的参数 仍然还是 会等于 0 0 所以我们会得到的是 所以我们会得到的是 对特定的电影 i 我们预测 Eve 的评分是 θ(5) 转置乘以 x(i) 

然后再加上 µi 所以如果 θ(5) 等于0的话 这第一部分就会等于0 所以对电影 i 的评分 我们最终会预测为 µi 这实际上是说得通的 它的意思是 对于电影 1 我们会预测 Eve 对它的评分是2.5 对于电影2 我们会预测 Eve 给它2.5星 对于电影3 我们会预测 Eve 给它2星 等等很多项 这其实说得通 因为它的意思是 如果 Eve 没给任何电影评分 我们就对这个新用户 Eve 一无所知 我们要做的就是预测 她对每个电影的评分 就是这些电影所得的平均评分 就是这些电影所得的平均评分 

最后再补充一下 在这个视频中 我们谈到了均值归一化 我们归一化矩阵Y 

使得每行的均值都是0 如果有些电影是没有评分的 这个情形类似于 有的用户没有给任何电影评分的情况 但是如果你有些电影 是没有评分的 你可以尝试这个算法的其他版本 你可以对不同的列 进行归一化 使得它们的均值为0 而不是把行均值归一化为0 虽说这个可能不太重要 因为如果你 真的有个电影没有评分 可能不管怎么说 你就不该把这个电影推荐给任何人 所以说 解决用户没评价过 任何电影的状况 可能比解决 电影没被评价过 的状况更重要 

最后总结一下 这就是可以说是作为协同过滤算法的预处理步骤 均值归一化的实现 根据你的数据集的不同 它可能有时会让实现的算法 表现得好一点儿 





### Review