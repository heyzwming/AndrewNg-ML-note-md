章节5 多变量线性回归(Multivariate Linear Regression))
===

## 课时27  多功能(Multiple Features)    08:22

假设我们有多个特征值和更多可以用来预测价格的信息，我们使用x_1 x_2 x_3 x_4来表示我们的四个特征，y来表示我们想要预测的房屋的输出变量(价格)

我们用
m：样本的数量
n：特征的数量
x^i：第i个训练样本的输入特征值(它是个向量，存放着所有特征量的值)
x_j^i：第i个训练样本中第j个特征量的值

![27.1](http://m.qpic.cn/psb?/V12umJF70r2BEK/ISAgi8az8axPHsP7Gi0PsScijNmY00nPWmL7xe4AFZ4!/b/dN0AAAAAAAAA&bo=LQRcAgAAAAARF1c!&rf=viewer_4)

我们有多个特征量和的假设形式应该是怎样的？
h_θ(x) = θ_0+θ_1 * x_1+θ_2 * x_2+θ_3 * x_3+θ_4 * x_4
![gongshi](http://m.qpic.cn/psb?/V12umJF70r2BEK/iH2O7kxks5kCD9jYHIecdvPDzoHALTQUiuAjPbyyy2Y!/b/dAsBAAAAAAAA&bo=yAJDAAAAAAARF6k!&rf=viewer_4)
![27.2](http://m.qpic.cn/psb?/V12umJF70r2BEK/U8.r9PTZg8ajelATxg5Tc6qpITOevWet*pCUcs.40Ww!/b/dN4AAAAAAAAA&bo=KQQ*AgAAAAARFzA!&rf=viewer_4)

为了符号方便，我们定义了额外的第0个特征向量x_0，并且它的取值总是1

由θ向量转置 内积 x向量的形式，为我们提供了一个便利的方式来表示假设，即用参数向量θ以及特征向量x的内积
![27.3](http://m.qpic.cn/psb?/V12umJF70r2BEK/osbaO999aOOp61Ju5GW*6B.K4Eb1LAx5l2lVwSj48XM!/b/dPQAAAAAAAAA&bo=SARAAgAAAAARBz4!&rf=viewer_4)

这就是在多特征量的情况下的假设形式，它就是所谓的多元线性回归(Multivariate linear regression)


## 课时28  多元梯度下降法(Gradient Descent for Multiple)    05:04


我们把参数θ看成是一个有关θ的 n+1维向量
J 看成 参数θ这个向量的函数
![28.1](http://m.qpic.cn/psb?/V12umJF70r2BEK/QW*fG3zKgqiQ7N.0rQ3pTfGZ1BGY4f.xf8OdptGQi94!/b/dN8AAAAAAAAA&bo=NQRwAgAAAAARB3M!&rf=viewer_4)
我们的梯度下降算法也得到了更新：用于多元线性回归的梯度下降算法
![28.2](http://m.qpic.cn/psb?/V12umJF70r2BEK/QW*fG3zKgqiQ7N.0rQ3pTfGZ1BGY4f.xf8OdptGQi94!/b/dN8AAAAAAAAA&bo=NQRwAgAAAAARF2M!&rf=viewer_4)

## 课时29  多元梯度下降法演练 I – 特征缩放(Gradient Descent in Practice Ⅰ - Feature Scaling)   08:52

你有一个机器学习问题，这个问题有多个特征，如果你能确保这些特征都在一个相近的范围，这样特度下降算法就可以更快地收敛。

也就是说，如果你的两个特征量(假设你有两个特征量)的值相差很大，那么，梯度下降算法收敛的速度可能会很慢，如下左图。一种有效的解决方法就是进行特征缩放,使得x1和x2都在0-1的范围，这样得到的梯度下降算法就会更快地收敛。

![29.1](http://m.qpic.cn/psb?/V12umJF70r2BEK/bBcOqAQC8Jm.cPIRwNTU4k*7Oh7ZI*hDFNJawS2B9c8!/b/dIIBAAAAAAAA&bo=jwUZAwAAAAARF7A!&rf=viewer_4)

当然 你认可的范围可以超过1和-1但别太大
![29.2](http://m.qpic.cn/psb?/V12umJF70r2BEK/fr5bUmTa4C5RgOJc0YxrJ0quzExKRi0X0fswlLwFU38!/b/dN4AAAAAAAAA&bo=eQUDAwAAAAARF1w!&rf=viewer_4)

除了将特征值除以最大值外，还有一种方法叫做 均值归一化(Mean normalization)
用x_i-μ_i(μ_i是特征值的取值平均值)来代替x_i，让你的特征值具有为0的平均值

更一般的，你可以把x1替换为(x1-μ1)/s1  (s1是该特征值的范围，max-min)，也可以把s1设为变量的标准差
![29.3](http://m.qpic.cn/psb?/V12umJF70r2BEK/YoxI7GVMauHBc4Q1j1Jr2j8Jt*R1qy98V4w.HH0D1Gk!/b/dN4AAAAAAAAA&bo=aQUdAwAAAAARB0I!&rf=viewer_4)

## 课时30  多元梯度下降法II – 学习率(Gradient Descent in Practice Ⅱ - Learning Rate ) 08:58

* 如何选择学习率α
* debug是什么
* 一些小技巧来确保梯度下降算法是正常工作的

![30.1](http://m.qpic.cn/psb?/V12umJF70r2BEK/oyxeOP.WTN1wuDk6zPiNDp0NjYz2qkfY.REsfS0KVIw!/b/dN8AAAAAAAAA&bo=GQREAgAAAAARF3s!&rf=viewer_4)

梯度下降算法所做的事情是为你找到一个θ值，并且希望它能够最小化代价函数J(θ)
> 我经常做的事是在算法运行的时候列出J的值

下图(代价函数J(θ)随迭代步数的变化曲线)的x轴式梯度下降算法的迭代次数，随着梯度下降算法的运行，你会得到如下的曲线，y轴就是经过x次迭代后得到的θ算出的J(θ)值。

如果在迭代进行过程中，每一步迭代之后J(θ)的值都应该在下降。
这条曲线可以帮助你判断梯度下降算法是否已经收敛。

不同的问题梯度下降算法的迭代步数可能大相径庭。

当然我们可以通过一个自动收敛测试算法来得到是否收敛的结果。当然还是看左边的图像更方便一点，因为可以直观看出算法有没有正常工作。
![30.2](http://m.qpic.cn/psb?/V12umJF70r2BEK/NGNj9hO0vUbP4wYOOLjw9GmcC8kouwFfiu8OCM2HLVY!/b/dGwBAAAAAAAA&bo=LQRFAgAAAAARF04!&rf=viewer_4)

图像的小Tips：

1、 上升
2、 正弦式变化

solution：减小α

![30.3](http://m.qpic.cn/psb?/V12umJF70r2BEK/oo5kgHZyYF4l9D9tQgdjIbct2gsuQH.5*Y56U3nvRng!/b/dN8AAAAAAAAA&bo=GARKAgAAAAARF3Q!&rf=viewer_4)

为了调试所有的情况，通常绘制J(θ)随迭代步数变化的曲线可以帮你弄清楚到底发生了什么

选择一个使得J(θ)快速下降的α值：在最大的α和极小的之间取最大偏小一点的。
![30.4](http://m.qpic.cn/psb?/V12umJF70r2BEK/GXBecEfVal5tAk6LNUHTox*Fn5Cct11u2KJ3AuKtg.A!/b/dPQAAAAAAAAA&bo=MgRvAgAAAAARB2s!&rf=viewer_4)

## 课时31  特征和多项式回归(Features and Polynomial)  07:39


## 课时32  正规方程（区别于迭代方法的直接解法）(Normal Equation)  16:17


## 课时33  正规方程在矩阵不可逆情况下的解决方法(Normal Equation Noninvertibility)  05:59


## 课时34  完成并提交编程作业(Working on and Submitting Programming Assignments)    03:33


