章节5 多变量线性回归(Multivariate Linear Regression))
===

## 课时27  多功能(Multiple Features)    08:22

假设我们有多个特征值和更多可以用来预测价格的信息，我们使用x_1 x_2 x_3 x_4来表示我们的四个特征，y来表示我们想要预测的房屋的输出变量(价格)

我们用
m：样本的数量
n：特征的数量
x^i：第i个训练样本的输入特征值(它是个向量，存放着所有特征量的值)
x_j^i：第i个训练样本中第j个特征量的值

![27.1](http://m.qpic.cn/psb?/V12umJF70r2BEK/ISAgi8az8axPHsP7Gi0PsScijNmY00nPWmL7xe4AFZ4!/b/dN0AAAAAAAAA&bo=LQRcAgAAAAARF1c!&rf=viewer_4)

我们有多个特征量和的假设形式应该是怎样的？
h_θ(x) = θ_0+θ_1 * x_1+θ_2 * x_2+θ_3 * x_3+θ_4 * x_4
![gongshi](http://m.qpic.cn/psb?/V12umJF70r2BEK/iH2O7kxks5kCD9jYHIecdvPDzoHALTQUiuAjPbyyy2Y!/b/dAsBAAAAAAAA&bo=yAJDAAAAAAARF6k!&rf=viewer_4)
![27.2](http://m.qpic.cn/psb?/V12umJF70r2BEK/U8.r9PTZg8ajelATxg5Tc6qpITOevWet*pCUcs.40Ww!/b/dN4AAAAAAAAA&bo=KQQ*AgAAAAARFzA!&rf=viewer_4)

为了符号方便，我们定义了额外的第0个特征向量x_0，并且它的取值总是1

由θ向量转置 内积 x向量的形式，为我们提供了一个便利的方式来表示假设，即用参数向量θ以及特征向量x的内积
![27.3](http://m.qpic.cn/psb?/V12umJF70r2BEK/osbaO999aOOp61Ju5GW*6B.K4Eb1LAx5l2lVwSj48XM!/b/dPQAAAAAAAAA&bo=SARAAgAAAAARBz4!&rf=viewer_4)

这就是在多特征量的情况下的假设形式，它就是所谓的多元线性回归(Multivariate linear regression)


## 课时28  多元梯度下降法(Gradient Descent for Multiple)    05:04


我们把参数θ看成是一个有关θ的 n+1维向量
J 看成 参数θ这个向量的函数
![28.1](http://m.qpic.cn/psb?/V12umJF70r2BEK/QW*fG3zKgqiQ7N.0rQ3pTfGZ1BGY4f.xf8OdptGQi94!/b/dN8AAAAAAAAA&bo=NQRwAgAAAAARB3M!&rf=viewer_4)
我们的梯度下降算法也得到了更新：用于多元线性回归的梯度下降算法
![28.2](http://m.qpic.cn/psb?/V12umJF70r2BEK/QW*fG3zKgqiQ7N.0rQ3pTfGZ1BGY4f.xf8OdptGQi94!/b/dN8AAAAAAAAA&bo=NQRwAgAAAAARF2M!&rf=viewer_4)

## 课时29  多元梯度下降法演练 I – 特征缩放(Gradient Descent in Practice Ⅰ - Feature Scaling)   08:52

你有一个机器学习问题，这个问题有多个特征，如果你能确保这些特征都在一个相近的范围，这样特度下降算法就可以更快地收敛。

也就是说，如果你的两个特征量(假设你有两个特征量)的值相差很大，那么，梯度下降算法收敛的速度可能会很慢，如下左图。一种有效的解决方法就是进行特征缩放,使得x1和x2都在0-1的范围，这样得到的梯度下降算法就会更快地收敛。

![29.1](http://m.qpic.cn/psb?/V12umJF70r2BEK/bBcOqAQC8Jm.cPIRwNTU4k*7Oh7ZI*hDFNJawS2B9c8!/b/dIIBAAAAAAAA&bo=jwUZAwAAAAARF7A!&rf=viewer_4)

当然 你认可的范围可以超过1和-1但别太大
![29.2](http://m.qpic.cn/psb?/V12umJF70r2BEK/fr5bUmTa4C5RgOJc0YxrJ0quzExKRi0X0fswlLwFU38!/b/dN4AAAAAAAAA&bo=eQUDAwAAAAARF1w!&rf=viewer_4)

除了将特征值除以最大值外，还有一种方法叫做 均值归一化(Mean normalization)
用x_i-μ_i(μ_i是特征值的取值平均值)来代替x_i，让你的特征值具有为0的平均值

更一般的，你可以把x1替换为(x1-μ1)/s1  (s1是该特征值的范围，max-min)，也可以把s1设为变量的标准差
![29.3](http://m.qpic.cn/psb?/V12umJF70r2BEK/YoxI7GVMauHBc4Q1j1Jr2j8Jt*R1qy98V4w.HH0D1Gk!/b/dN4AAAAAAAAA&bo=aQUdAwAAAAARB0I!&rf=viewer_4)

## 课时30  多元梯度下降法II – 学习率(Gradient Descent in Practice Ⅱ - Learning Rate ) 08:58



## 课时31  特征和多项式回归(Features and Polynomial)  07:39


## 课时32  正规方程（区别于迭代方法的直接解法）(Normal Equation)  16:17


## 课时33  正规方程在矩阵不可逆情况下的解决方法(Normal Equation Noninvertibility)  05:59


## 课时34  完成并提交编程作业(Working on and Submitting Programming Assignments)    03:33


