十三、Unsupervised Learning(无监督学习)
===
## Clustering(聚类)
---

## 13.1、 Unsupervised Learning:Introduction(无监督学习：介绍)

接下来的3分钟，我将介绍 集群 这一概念。 

保证精彩。 因为这是我们第一个 无监督学习算法。 我们要从未标记的数据中进行学习, 而不是从已标记的数据。 

那么, 何谓 无监督学习算法 ? 

之前在本课程的开始阶段， 我曾简短介绍无监督学习算法。 现在，我想将 无监督学习算法 与 监督学习算法 做个对照。 

首先，屏幕上是一个典型的监督学习例子。 有一组附标记的训练数据集， 我们想要找出 决策边界，藉此区分开 正(positive)或负(negative)标记数据。 

在这种 监督式学习 案例中。 我们针对一组附标记的训练数据 提出一个适当的假设。 相比之下，在无监督学习案例中。 我们面对的是一组无标记的训练数据， 数据之间， 不具任何相关联的标记。 

所以我们得到的数据 看起来像这样：一个数据集， 一堆数据点，但没有任何标记以供参考。 

所以从训练数据中， 我们只能看到 x 1、 x 2... 等等... 到 x(m) 没有任何标记 y 供参考。 这就是为什么 在图上我们看不到 任何标记 y。 所以在无监督学习中， 我们将这种 未标记的训练数据送入 特定的算法， 然后我们要求算法 替我们分析出数据的结构。 

就此数据而言， 其中一种可能的结构 是 所有的数据 可以大致地划分成 两个类或组。 因此，像我介绍的 这种划分组的算法， 称为 

聚类算法。 这是我们第一种 无监督学习算法。 除此之外， 无监督学习还包含 其他各式各样算法， 用以寻找其他类型的结构， 我们将 一一提及。 目前，我只讨论 聚类 。 

那么， 聚类 功用为何？ 稍早前，我已经提到几个应用实例： 一是细分市场(market segmentation)。 假设有一个客户数据库， 想要将所有客户划分 至不同的细分市场组， 以便于营销 或服务。 

这在社交分析体系中， 已经存在多年了。 比如， 观察一群人，社交网络 像 Facebook、 Google+ 或者有关 个人的信息... 你通常和谁有电子邮件来往，. 他们又和谁有电子邮件来往。. 或者 查找一群相互有联系的人。 

这是另一种 聚类算法 。 你会想要在一个社交网络中 查找那些其他相互有关联的朋友。 以我一位朋友 在做的事为例： 他使用聚类 来组织运算集群或组织数据中心， 因为，如果你知道 在集群中，哪些计算机 的数据中心倾向于一起工作。 你可以用它重新组织 您的资源， 网络的布局， 数据中心和通信。 

最后， 一件我正在从事的工作。 我使用聚类算法， 试图理解星系的形成 和其中的 天文的细节。 

总而言之，聚类 ， 是我们无监督学习算法的 第一个例子。 在接下来的视频中，我们将开始 谈论具体的聚类算法。 


## 13.2、 K-Means Algorithm(K-Means 算法)

在聚类问题中 我们有未加标签的数据 我们希望有一个算法 能够自动的 把这些数据分成 有紧密关系的子集或是簇 

K均值 (K-means) 算法 是现在最为广泛使用的 聚类方法 那么在这个视频中 我将会告诉你 什么是K均值算法以及它是怎么运作的 

K均值算法最好用图来表达 如图所示 现在我有一些 没加标签的数据 而我想将这些数据分成两个簇 

现在我执行K均值算法 方法是这样的 首先我随机选择两个点 这两个点叫做聚类中心 (cluster centroids) 就是图上边的两个叉 这两个就是聚类中心 

为什么要两个点呢 因为我希望聚出两个类 

K均值是一个迭代方法 它要做两件事情 

第一个是簇分配 第二个是移动聚类中心 我来告诉你这两个是干嘛的 

在K均值算法的每次循环中 第一步是要进行簇分配 这就是说 我要遍历所有的样本 就是图上所有的绿色的点 然后依据 每一个点 是更接近红色的这个中心 还是蓝色的这个中心 来将每个数据点 分配到两个不同的聚类中心中 

具体来讲 我指的是 对数据集中的所有点 依据他们 更接近红色这个中心 还是蓝色这个中心 进行染色 染色之后的结果如图所示 

以上就是簇分配的步骤 

K均值的另一部分 是要移动聚类中心 具体的操作方法 是这样的 我们将两个聚类中心 也就是说红色的叉 和蓝色的叉 移动到 和它一样颜色的那堆点的均值处 那么我们要做的是 找出所有红色的点 计算出它们的均值 就是所有红色的点 平均下来的位置 然后我们就把红色点的聚类中心移动到这里 蓝色的点也是这样 找出所有蓝色的点 计算它们的均值 把蓝色的叉放到那里 那我们现在就这么做 我们将按照图上所示这么移动 

现在两个中心都已经移动到新的均值那里了 你看 蓝色的这么移动 红色的这么移动 然后我们就会进入下一个 簇分配 我们重新检查 所有没有标签的样本 依据它离红色中心还是蓝色中心更近一些 将它染成红色或是蓝色 我要将每个点 分配给两个中心的某一个 就像这么做 

你看某些点的颜色变了 

然后我们又要移动聚类中心 于是我计算 蓝色点的均值 还有红色点的均值 然后就像图上所表示的 移动两个聚类中心 来我们再来一遍 下面我还是要做一次簇分配 将每个点 染成红色或是蓝色 依然根据它们离那个中心近 然后是移动中心 你看就像这样 实际上 如果你从这一步开始 一直迭代下去 聚类中心是不会变的 并且 那些点的颜色也不会变 在这时 我们就能说 K均值方法已经收敛了 在这些数据中找到两个簇 

K均值表现的很好 来我们用更加规范的格式描述K均值算法 

K均值算法接受两个输入 第一个是参数K 表示你想从数据中 聚类出的簇的个数 我一会儿会讲到 我们可以怎样选择K 这里呢 我们只是说 我们已经确定了 需要几个簇 然后我们要告诉这个算法 我们觉得在数据集里有多少个簇 K均值同时要 接收另外一个输入 那就是只有 x 的 没有标签 y 的训练集 因为这是非监督学习 我们用不着 y 同时在非监督学习的 K均值算法里 我们约定 x(i) 是一个n维向量 这就是 训练样本是 n 维而不是 n+1 维的原因 

这就是K均值算法 

第一步是 随机初始化 K 个聚类中心 记作 μ1, μ2 一直到 μk 就像之前 图中所示 聚类中心对应于 红色叉和蓝色叉 所在的位置 于是我们有两个聚类中心 按照这样的记法 红叉是 μ1 蓝叉是 μ2 通常情况下 我们可能会有比2要多的聚类中心 K均值的内部循环 是这样的 我们会重复做下面的事情 

首先 对于每个训练样本 我们用变量 c(i) 表示 K个聚类中心中最接近 x(i) 的 那个中心的下标 这就是簇分配 这个步骤 我先将每个样本 依据它离那个聚类中心近 将其染成 红色或是蓝色 所以 c(i) 是一个 在1到 K 之间的数 而且它表明 这个点到底是 更接近红色叉 还是蓝色叉 

另一种表达方式是 我想要计算 c(i) 那么 我要用第i个样本x(i) 然后 计算出这个样本 

距离所有K个聚类中心的距离 这是 μ 以及小写的k 大写的 K 表示 所有聚类中心的个数 小写的 k 则是 不同的中心的下标 

我希望的是 在所有K个中心中 找到一个k 使得xi到μk的距离 是xi到所有的聚类中心的距离中 最小的那个 也就是说 k的值使这个最小 这就是计算ci的方法 这里还有 另外的表示ci的方法

我用xi减μk的范数 来表示 

这是第i个训练样本 到聚类中心μk 的距离 注意 我这里用的是小写的k 大写的K 大写的k表示 聚类中心的总数 这个小写的k 是第一个到第K个中心 中的一个 我用小写的k 表示不同聚类中心的下标 

这是个小写k 

这就是某个样本到聚类中心的距离 接下来 我要做的是 找出小写的k的值 让这个式子最小 那么 接下来 我就要将 c(i) 赋值为k 我这里按照惯例表示 x(i) 和聚类中心的距离 因为出于惯例 人们更喜欢用距离的平方来表示 所以我们可以认为 c(i) 是距样本 x(i) 的距离的平方 最小的那个聚类中心 当然 使距离的平方最小或是距离最小 都能让我们得到相同的 c(i) 但是我们通常还是 写成距离的平方 因为这是约定俗成的 这就是簇分配 

K均值循环中的另一部分是 移动聚类中心 

这是说 对于每个聚类中心 也就是说 小写k从1循环到K 将 μk 赋值为这个簇的均值 举个栗子 某一个聚类中心 比如说是 μ2 被分配了一些训练样本 像是1,5,6,10 这个表明 c(1) 等于2 

c(5) 等于2 

c(6) 等于2 同样的 c(10) 也是等于2 对吧? 

如果我们从上一步 也就是簇分配那一步得到了这些 这个表明 样本1 5 6 10被分配给了聚类中心2 

然后在移动聚类中心这一步中 我们要做的是 计算出这四个的平均值 

即 计算 x(1)+x(5)+x(6)+x(10) 然后计算 它们的平均值 这里聚类中心有 4个点 那么我们要计算和的四分之一 这时μ2就是一个 n维的向量 因为 x(1) x(5) x(6) x(10) 都是 

n维的向量 然后 把这些相加 再除以4 因为 有4个点分配到了这个聚类中心 这样聚类中心μ2的移动 

就结束了 这个作用是说 将μ2移动到 这四个点的均值处 

我要问的问题是 既然我们要让μk移动到分配给它的那些点的均值处 那么如果 存在一个 没有点分配给它的聚类中心 那怎么办? 通常在这种情况下 我们就直接移除 那个聚类中心 如果这么做了 最终将会得到K-1个簇 

而不是K个簇 如果就是要K个簇 不多不少 但是有个 没有点分配给它的聚类中心 你所要做的是 重新随机找一个聚类中心 但是直接移除那个中心 是更为常见的方法 当你遇到了一个 没有分配点的 聚类中心 不过在实际过程中 这个问题不会经常出现 这就是K均值算法 

在这个视频结束之前 我还想告诉你 K均值的 另外一个常见应用 应对没有很好分开的簇 
1
比如说 到目前为止 我们的K均值算法 都是基于一些像图中所示的数据 有很好的隔离开来的 三个簇 然后我们就用这个算法找出三个簇 但是事实是 K均值经常会用于 一些这样的数据 看起来并没有 很好的分来的 几个簇 这是一个应用的例子 关于T恤的大小 
1
假设你是T恤制造商 你找到了一些人 想把T恤卖给他们 然后 你搜集了一些 这些人的 身高和体重的数据 我猜 身高体重更重要一些 然后你可能 收集到了这样的样本 一些关于 人们身高和体重的样本 就像这个图所表示的 然后你想确定一下T恤的大小 假设我们要设计 三种不同大小的t恤 小号 中号 和大号 那么小号应该是多大的? 中号呢? 大号呢? 
1
有一种 在这样的数据上 使用K均值算法进行聚类 的方法就像我展示的那样 而且可能 K均值可能将这些 聚成一个簇 把这些点 聚成第二个簇 然后把这些点 聚成第三个簇 所以说 尽管这些数据 原本看起来并没有 三个分开的簇 但是从某种程度上讲 K均值仍然能将数据分成几个类 然后你能做的就是 看这第一群人 然后 查看他们的 身高和体重 试着去设计 对这群人来说 比较合身的小号衣服 以及设计一个中号的衣服 设计一个大号的衣服 这就是一种 市场细分的例子 
1
当你用K均值方法 将你的市场分为三个不同的部分 你就能够区别对待 你三类不同的顾客群体 
1
更好的适应 他们不同的需求 就像大中小三种不同大小的衣服那样 这就是K均值算法 而且你现在应该 已经知道如果去实现 K均值算法并且利用它解决一些问题 在下面的视频中 我想把K均值算法 研究的更深入一些 然后讨论一下 如何能让K均值表现得更好一些的问题



## 13.3、 Optimization Objective(优化目标)

在大多数我们已经学到的 监督学习算法中 类似于线性回归 逻辑回归 以及更多的算法 所有的这些 算法都有一个优化目标函数 或者某个代价函数需要通过算法进行最小化 

事实上 K均值也有 一个优化目标函数或者 需要最小化的代价函数 在这个视频中 我会 告诉大家这个优化目标函数是什么 我这么做有两方面的目的 具体来说 

首先 了解什么是 K均值的优化目标函数 这将能帮助我们 调试学习算法 确保K均值算法 是在正确运行中 第二个也是最重要的一个目的是 在之后的视频中我们将讨论 我们该怎样运用这个来 帮助K均值找到更好的簇 并且避免局部最优解 不过我们在这节课之后的视频中 才会涉及到 

另外顺便提一下 当K均值正在运行时 

我们将对两组变量进行跟踪 首先是 c(i) 它表示的是 当前的样本 x(i) 所归为 

的那个簇的索引或者序号 另外一组变量 我们用 μk 来表示 第 k 个簇的  聚类中心 (cluster centroid) 顺便再提一句 K均值中我们用大写 K  来表示簇的总数 用小写 k 来表示 聚类中心的序号 因此 小写 k 的范围 就应该是1到大写K之间 除此以外  还有另一个符号 我们用 μc(i) 来表示 x(i) 所属的那个簇 的聚类中心 我再稍微多解释一下 这个符号 假如说 x(i) 被划为了 第5个簇 

这是什么意思呢？ 

这个意思是 x(i) 的序号 也就是 c(i) 等于5 因为 c(i) = 5 表示的就是 

x(i) 这个样本 被分到了第五个簇 因此  μ 下标 c(i) 就等于 μ5  因为 c(i) 就是5 

所以 

这里的 μc(i) 就是第5个簇的聚类中心 而也正是我的样本 x(i) 所属的第5个簇 有了这样的符号表示 现在我们就能写出 K均值聚类算法的 优化目标了 以下便是 K均值算法需要 最小化的代价函数 J 参数是 c(1) 到 c(m) 以及 μ1 到 μk 随着算法的执行过程 这些参数将不断变化 右边给出了优化目标 也就是所有的 1/m 乘以 i = 1 到 m 个项的求和 

这里我用红色框出了这部分 也即每个样本 x(i) 到 x(i) 所属的 聚类簇的中心 距离的平方值 

下面 

我来解释一下 这是训练样本 x(i) 的位置 这是 x(i) 这个样本被划分到的 聚类簇的中心的位置 我们在图上解释一下 如果这是 x1 x2 并且如果这个点 是我的第 i 个样本  x(i) 那么 也就是说这个值等于 x(i) 

并且 x(i) 被分到了 某一个聚类中心 我用一个叉来表示这个聚类中心 所以 如果我们假设 这个聚类中心是 μ5  也就是说 假如 x(i) 被分到第五个聚类簇 那么这个距离平方值 也就是点 x(i) 

和 x(i) 被分配到的聚类中心的 距离的平方值 

那么 K均值算法 要做的事情就是 它将找到参数 c(i) 和 μi 也就是说 找到能够最小化 代价函数 J 的 c 和 μ 这个代价函数 在K均值算法中 有时候也叫做 失真代价函数(distortion cost function) 再解释详细点 这是K均值算法 这跟我们之前得到的 算法是一样的 这个算法的第一步 就是聚类中心的分配 在这一步中 

我们要把每一个点 划分给各自所属的聚类中心 可以用数学证明 这个聚类簇的划分步骤 实际上就是在 

对代价函数 J 进行最小化 关于参数 c(1) c(2)  等等 一直到 c(m) 而保持最近的聚类中心 μ1 到 μk 固定不变 

因此 第一步要做的 其实不是改变 聚类中心的位置 而是选择 c(1) c(2) 一直到 c(m) 

来最小化这个代价函数 或者说失真函数 J 不难从数学的角度证明 但我在这里就不做了 但应该还是比较容易理解 这个过程就是把这些点 划分到离它们最近的 那个聚类中心 因为这样才会使得 点到对应聚类中心的距离最短 然后 另一部分 K-均值算法的第二步 

这一部分的任务是 聚类中心的的移动 我依然不会证明这一步 但实际上是可以 在数学上来证明这一点 也就是说这一步 是选择了能够 最小化 J 的 μ 的值 也就是说 最小化代价函数 J 关于 这里的 wrt 表示 with respect to (关于) 因此是最小化 J 关于所有聚类中心的位置 μ1 到 μK 因此 K均值算法 实际上是把这两组变量 在这两部分中 分割开来考虑 分别最小化 J 首先是 c 作为变量 然后是 μ 作为变量 那么 K均值的工作就是 首先关于 c 求 J 的最小值 然后关于 μ  求 J 的最小值 然后反复循环 

这就是 K均值算法 

现在 我们已经理解了 K均值算法的原理 就是最小化代价函数 J 的过程 我们也可以用这个原理 来试着调试我们的学习算法 保证我们对 K均值算法的实现过程 是正确的 

好的 这节课我们介绍了 K均值算法 其核心就是 对代价函数 J 的优化过程 代价函数 J 也被称为失真函数 

我们可以用这个知识 来调试K均值算法 证明算法是否正在收敛 是否正在正常工作 在下一节视频中 我们将一起看看 如何帮助K均值找到更好的簇 同时避免局部最优解 【教育无边界字幕组】翻译：柳桦 校对/审核：所罗门捷列夫 


## 13.4、 Random Initialization(随机初始化)

在本节课的视频中 讨论一下如何初始化 

K均值聚类方法 更重要的是 这将 引导我们讨论 如何避开局部最优来构建K均值聚类方法 这是一个我们之前讨论过的 K均值聚类算法 

其中我们之前没有 讨论得太多的是这一步 如何初始化聚类中心这一步 有几种不同的方法 可以用来随机 初始化聚类中心 但是 事实证明 有一种方法比其他 大多数可能考虑到的方法 更加被推荐 接下来就告诉你这个 方法 因为它可能是效果最好的一种方法 

这里展示了我通常是如何初始化我的聚类中心的 

当运行K均值方法时 你需要有 一个聚类中心数值K K值要比 训练样本的数量m小 如果运行一个 K均值 聚类中心数值 等于或者大于样本数的K均值聚类方法会很奇怪 

我通常用来 初始化K均值聚类的方法是 随机挑选K个训练 样本 然后 我要做的是设定μ1 到μk让它们等于这个K个样本 

让我展示一个具体的例子 

我们假设 K 等于2 那么 就在这个例子的右边 假设我们想找到两个聚类 

那么为了初始化 聚类中心  我要做的是 随机挑选几个样本 比如说 我挑选了 这个和这个 我要 初始化聚类中心的方法 就是 我只需要初始化 

聚类中心正确的样本中 因此这是我的第一个聚类中心 这是我的第二个聚类中心 这就是一个随机初始化K均值聚类的方法 

刚刚我画的看上去是相当不错的一个例子 但是有时候我可能不会 那么幸运 也许我最后 会挑选到 这一个是我第一个 挑选到的初始化样本 而这是第二个 这就是我所挑选的两个样本 因为K等于2 我们随机挑选了两个 训练样本 如果 我挑选这两个 那么我 结果就有可能得到 这个是第一个聚类 中心 这个是 第二个聚类中心 这就是如何随机 初始化聚类中心 因此在初始化时 你的第一个 聚类中心μ1 等于x(i) 对于某一个随机的i值 

μ2等于x(j) 

对应另一个随机选择的不同的 j的值 等等 如果你有更多的聚类和更多的聚类中心的话 

通常 应该这样说 前面的视频中 在我第一次 用动画说明K均值方法时 

在那些幻灯片中 仅仅是为了说明 我实际上用了一种不同的 初始化方法来初始化聚类中心 而这张幻灯片中描述的方法 是真正被推荐的方法 这种方法在你实现K均值聚类的时候可能会用到 

根据推荐 也许通过这右边的两个图 你可能会猜到K均值方法 最终可能会得到 不同的结果 取决于 聚类簇的初始化方法 因此也就取决于随机的初始化 

K均值方法最后可能得到不同的结果 尤其是如果K均值方法落在局部最优的时候 

如果给你一些数据 比如说这些 这看起来好像有 3个聚类 那么 如果你运行K均值方法 如果 它最后得到一个 局部最优 这可能是 真正的全局最优 你可能会得到这样的聚类结果 但是如果你运气特别 不好 随机初始化 K均值方法 也可能会卡在不同的 局部最优上面 因此在 有变的这个例子中 看上去蓝色的聚类捕捉到了 左边的很多点 而且它们在绿色的聚类中 每一个聚类都捕捉到了相对较少的点 这与 不好的局部最优相对应 因为 它基本上是基于这两个 聚类的 并且它们 进一步合并成了1个聚类 而 把第二个聚类分割成了 两个像这样的小的聚类 它也把 第二个聚类 分割成了两个 分别的像这样的小聚类簇 这两个 右下方的例子 对应与K均值方法的 不同的局部最优 实际上 这里的这个例子 这个红色的簇 只捕捉到了一个最好的样本 这个局部 最优项 顺便提一下 代表 这个失真函数J 的局部最优 这些在右下方的解 这些局部 最优所对应的是 真正的K均值方法 所遇到的局部 最优 且 通过最小化这个 失真函数J并不能得到很好的结果 因此 如果你担心K均值方法会遇到 局部最优的问题 如果 你想提高 K均值方法找到最 有可能的聚类的几率的话  就像这上面所展示的 我们能做的 是尝试多次 随机的初始化 而不是仅仅初始化一次K均值方法 就希望它会得到 很好的结果 我们能做的是 初始化K均值很多次 并运行K均值方法很多次 通过多次尝试 来保证我们最终能得到 一个足够好的结果 一个 尽可能局部或全局最优的结果 

具体地 这就是你能够做的 假如我决定运行 K均值方法一百次 那么我就需要执行这个循环 100次 这是一个相当典型的次数数字 有时会是 从50到1000之间 

假设说有决定运行K均值方法100次 

那么这就意味这 我们要随机初始化K均值方法 对于这些 100次随机初始化的每一次 我们需要运行K均值方法 我们会得到一系列 聚类结果 和一系列聚类 中心 之后 我们可以计算失真函数J 用我们得到的 

这些聚类结果 和聚类中心来计算这样一个结果函数 

最后 完成整个过程100次之后 你会得到这个100种 聚类数据的这些方法  最后你要做的是 在所有这100种 用于聚类的方法中 选取能够给我们代价最小的一个 给我们最低畸变值的一个 事实证明 如果你运行K均值方法时 所用的聚类数相当小 那么如果聚类 数是从 2到10之间的任何数的话 做多次的随机初始化 通常能够保证 你能有一个较好的局部最优解 保证你能找到更好的聚类数据 但是如果K非常大的话 如果 K比10大很多 当然如果K是 如果你尝试去 找到成百上千个聚类 那么 

有多个随机初始化就 不太可能会有太大的影响 更有 可能你的第一次 随机初始化就会给 你相当好的结果 

做多次随机 初始化可能会给 你稍微好一点的结果 但是不会好太多 但是在这样一个 聚类数相对较小的体系里 特别是如果你 有2个或者3个 或者4个聚类的话 随机 初始化会有较大的影响 可以保证你在 最小化失真函数的时候得到一个很小的值 并且能得到一个很好的聚类结果 

这就是随机初始化 的K均值初始化方法 

如果你尝试学习一种 聚类数目相对较小 的聚类方法 如2,3 4,5,6,7 用 

多次随机初始化 有时能够帮助你找到更好的数据聚类结果 但是 尽管你有很多聚类数目 初始化 我在这里介绍的随机初始化 它会给K均值方法一个 合理的起始点来开始 并找到一个好的聚类结果 

## 13.5、 Choosing the Number of Clusters(选取聚类数量)

在这段视频中  我想讨论一下 K-均值聚类的最后一个细节 我想选择聚类数目的更好方法是 或者说是如何去选择 参数大写K的值 

说实话 这个问题上没有一个 非常标准的解答 或者能自动解决它的方法 目前用来决定聚类数目的 最常用的方法 仍然是通过看可视化的图 或者看聚类算法的输出结果 或者其他一些东西来手动地决定聚类的数目 

但是 我确实经常被别人问到 这样的问题 你是如何来选择聚类的数目的 我只能告诉你一些 人们现在对这个问题的思考 人们现在对这个问题的思考 尽管最为常见的方法 实际上仍是手动选择 聚类的数目 

选择聚类的数目 可能不总是那么容易 可能不总是那么容易 大部分原因是 数据集中有多少个聚类通常是模棱两可的 

看到这样一个数据集 有些人可能会看到 四个聚类 那么这就意味着需要使用 K=4 或者有些人可能 会看到两个聚类 这就意味着 K=2 可能其他人会看到3个聚类 

那么 看到这样一个数据集 那么 看到这样一个数据集 在我看来它的真实的类别数 实际上确实是模棱两可的 所以我并不认为这里有一个正确答案 这就是无监督学习的一部分 没有给我们标签 所以不会总有一个清晰的答案 这是为什么 做一个能够自动选择 聚类数目的算法 是非常困难的原因之一 

当人们讨论 选择聚类数目的方法时 可能会提及一个叫做 肘部法则 (Elbow Method) 的方法 现在我来介绍一下它 之后会提及到它的一些优点和缺点 那么对于肘部法则 我们所需要做的是改变K的值 也就是聚类类别的总数 我们用K值为1来运行K-均值聚类算法 这就意味着 所有的数据都会分到一个类里 然后计算代价函数 或者说计算畸变J 并将其画在这儿 然后我们选用两个聚类 来运行K-均值聚类算法 可能用了多个随机的初始中心 也可能没用 那么 有两个聚类的话 我们很可能得到 一个较小的畸变值 

把它画在这儿 然后用三个聚类来运行K-均值聚类 你很有可能得到更小的畸变值 把它画在这儿 之后再让聚类数目等于4 5来运行K-均值聚类 最后我们就能 得到一条曲线 它展示了随着聚类数量的增多 畸变值是如何下降的 我们可能会得到一条这样的曲线 

看到这条曲线 肘部法则会说 “我们来看这个图 这里看起来是一个很清楚的肘点” 对吧 这就类比于人的手臂 对吧 这就类比于人的手臂 如果你想象一下 伸出你的手臂 那么这就是你的肩关节 这就是你的肘关节 我想你的手就在末端这里 这就是肘部法则 你会发现这样一种模式 K从1变化到2 再从2到3时 畸变值迅速下降 然后在3的时候 到达一个肘点 此后畸变值就下降得非常慢 这样看起来 也许使用3个类 是聚类数目的正确选择 这是因为那个点 是曲线的肘点 就是说畸变值快速地下降 直到K等于3这个点 在这之后就下降得非常慢 那么我们就选K等于3 

当你应用肘部法则的时候 如果你得到了一个 像这样的图 那么这非常好 这是一种用来 选择聚类个数的合理方法 

而事实证明肘部法则 并不那么常用 其中一个原因是 如果你把这种方法 用到一个聚类问题上 事实上你最后得到的曲线 通常看起来 是更加模棱两可的 也许就像这样 如果你看这条曲线 我不知道 也许没有一个清晰的肘点 而畸变值像是连续下降的 也许3是一个好选择 也许4是一个好选择 也许5也不差 那么如果 你在实际应用中使用了这个 如果你的图像左边这个的话 那么就太好了 它给你一个清晰的答案 但是通常来说 你最后得到的图 是像右边的这样的 肘点的位置 并不明确 这使得用这个方法 来选择聚类数目变得较为困难 简单小结一下肘部法则 它是一个值得尝试的方法 但是我不会期待 

它在任何问题上都有很高的表现 它在任何问题上都有很高的表现 

最后 有另外一种方法 来考虑如何选择K的值 来考虑如何选择K的值 通常人们使用 K-均值聚类算法 是为了某些后面的用途 或者说某种下游的目的 而要求得一些聚类 也许你会用K-均值聚类算法 来做市场分割 如我们之前谈论的T恤尺寸的例子 也许你会用K-均值聚类来让 电脑的聚类变得更好 或者可能为了某些别的目的学习聚类 等等 或者可能为了某些别的目的学习聚类 等等 如果那个后续 下游的目的 比如市场分割 那能给你一个评估标准 那么通常来说 决定聚类数量的 更好的办法是 看不同的聚类数量能为 后续下游的目的提供多好的结果 

我们来看一个具体的例子 

我们再看一下 T恤尺寸这个例子 我想要决定 我是需要3种T恤尺寸么吗？ 所以我选择 K=3 我可能有小号 中号 大号三类T恤 或者我可以选择 K=5 或者我可以选择 K=5 那么我可能有 特小号 小号 中号 大号 和特大号尺寸的T恤 所以 你可能有3种 4种 或者5种T恤尺寸 我们也可以有4种T恤尺寸 但是我为了使这张幻灯片更简洁一些 但是我为了使这张幻灯片更简洁一些 只展示3种和5种这两个情况 

因此如果我用 K=3 来运行K-均值聚类 我得到的结果可能是 

这是小号 这是中号 这是大号 这是小号 这是中号 这是大号 

然而 如果我用5个聚类 来运行K-均值聚类的话 我得到的结果可能是 那些是超小号T恤 这些是小号 那些是超小号T恤 这些是小号 这些是中号 这些是大号 这些是超大号 

这个例子的好处是 它会给我们 选择聚类数目是3 4还是5 选择聚类数目是3 4还是5 的另一种方法 

具体来说 你要做的是 从T恤生意的角度 来思考这个事情 然后问 “如果我有5个分段  那么我的T恤有多适合我的顾客？  那么我的T恤有多适合我的顾客？ 我可以卖出多少T恤？ 我的顾客将会有多高兴呢？” 从T恤生意的角度去考虑 其中真正有意义的是 也就是我是需要 更多的T恤尺寸 来更好地满足我的顾客 还是说我需要 更少的T恤尺寸 我制造的T恤尺码就更少 我就可以将它们更便宜地卖给顾客 因此T恤的销售业务的观点 可能会提供给你一个 决定采用3个类还是5个类的方法 

以上就是一个例子 后续下游目的 比如这个问题里的 决定生产什么样的T恤 怎样给我们一个 选择聚类数量的评价标准 对于那些在做 编程练习的同学来说 你去看一下这周的 K-均值聚类相关的编程练习 那是一个将K-均值用于图片压缩的例子 如果你想要选择 对这个问题 应该使用多少个聚类 我们可以再一次用 图片压缩的评估标准 来选择聚类数目K 你想要图片 看起来有多好 还是你想要将图片 压缩到多小 如果你做编程练习的话 到那时就能理解 我刚才所说的了 

总结一下 大部分时候  聚类数目仍然是通过 手动 人工输入或我们的洞察力来决定 一种可以尝试的方法是 使用肘部法则 使用肘部法则 但是我不会总是 期望它能表现得好 我想选择聚类数目的更好方法是 我想选择聚类数目的更好方法是 去问一下你运行K-均值聚类 是为了什么目的？ 

然后想一想 聚类的数目是多少 才适合你运行K-均值聚类的后续目的 才适合你运行K-均值聚类的后续目的

### Review